{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Main.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"z4zhE16p2LjR","colab_type":"code","outputId":"410dfe20-30e9-436c-be9b-ee84b4fcf8ec","executionInfo":{"status":"ok","timestamp":1567603642292,"user_tz":-330,"elapsed":13323,"user":{"displayName":"ABHISHEK shrivastava","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBrnGEKJvti9aSC93hdUbcjWUrO4DT-zsrX8DGL=s64","userId":"09294294585314968603"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["!git clone https://github.com/placibo/LSTM.git"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'LSTM'...\n","remote: Enumerating objects: 56, done.\u001b[K\n","remote: Counting objects: 100% (56/56), done.\u001b[K\n","remote: Compressing objects: 100% (46/46), done.\u001b[K\n","remote: Total 56 (delta 22), reused 33 (delta 9), pack-reused 0\u001b[K\n","Unpacking objects: 100% (56/56), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ohBrt_L42ZHa","colab_type":"code","colab":{}},"source":["import numpy as np               #for maths\n","import pandas as pd              #for data manipulation\n","import matplotlib.pyplot as plt  #for visualization"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iOT36Bjjd2vQ","colab_type":"code","colab":{}},"source":["path = r'./LSTM/input/NationalNames.csv'\n","data = pd.read_csv(path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vX26Q0N8eHet","colab_type":"code","colab":{}},"source":["data['Name'] = data['Name']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zVMnCidef6Md","colab_type":"code","colab":{}},"source":["data = np.array(data['Name'][:10000]).reshape(-1,1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-dCFLXA5f9MD","colab_type":"code","colab":{}},"source":["data = [x.lower() for x in data[:,0]]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tByAErlegMdt","colab_type":"code","colab":{}},"source":["data = np.array(data).reshape(-1,1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BClUATmpgTUu","colab_type":"code","colab":{}},"source":["transform_data = np.copy(data)\n","\n","#find the max length name\n","max_length = 0\n","for index in range(len(data)):\n","    max_length = max(max_length,len(data[index,0]))\n","    \n","#make every name of max length by adding '.'\n","for index in range(len(data)):\n","    length = (max_length - len(data[index,0]))\n","    string = '.'*length\n","    transform_data[index,0] = ''.join([transform_data[index,0],string])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r9ALqQtYb9tx","colab_type":"code","colab":{}},"source":["vocab = list()\n","for name in transform_data[:,0]:\n","    vocab.extend(list(name))\n","\n","vocab = set(vocab)\n","vocab_size = len(vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QU0dWluzcF25","colab_type":"code","colab":{}},"source":["#map char to id and id to chars\n","char_id = dict()\n","id_char = dict()\n","\n","for i,char in enumerate(vocab):\n","    char_id[char] = i\n","    id_char[i] = char\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0r0LNNLycJCY","colab_type":"code","colab":{}},"source":["# list of batches of size = 20\n","train_dataset = []\n","\n","batch_size = 20\n","\n","#split the trasnform data into batches of 20\n","for i in range(len(transform_data)-batch_size+1):\n","    start = i*batch_size\n","    end = start+batch_size\n","    \n","    #batch data\n","    batch_data = transform_data[start:end]\n","    \n","    if(len(batch_data)!=batch_size):\n","        break\n","        \n","    #convert each char of each name of batch data into one hot encoding\n","    char_list = []\n","    for k in range(len(batch_data[0][0])):\n","        batch_dataset = np.zeros([batch_size,len(vocab)])\n","        for j in range(batch_size):\n","            name = batch_data[j][0]\n","            char_index = char_id[name[k]]\n","            batch_dataset[j,char_index] = 1.0\n","     \n","        #store the ith char's one hot representation of each name in batch_data\n","        char_list.append(batch_dataset)\n","    \n","    #store each char's of every name in batch dataset into train_dataset\n","    train_dataset.append(char_list)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sVPPX-nEcNew","colab_type":"code","colab":{}},"source":["#number of input units or embedding size\n","input_units = 100\n","\n","#number of hidden neurons\n","hidden_units = 256\n","\n","#number of output units i.e vocab size\n","output_units = vocab_size\n","\n","#learning rate\n","learning_rate = 0.005\n","\n","#beta1 for V parameters used in Adam Optimizer\n","beta1 = 0.90\n","\n","#beta2 for S parameters used in Adam Optimizer\n","beta2 = 0.99"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pABj827jcQar","colab_type":"code","colab":{}},"source":["#Activation Functions\n","#sigmoid\n","def sigmoid(X):\n","    return 1/(1+np.exp(-X))\n","\n","#tanh activation\n","def tanh_activation(X):\n","    return np.tanh(X)\n","\n","#softmax activation\n","def softmax(X):\n","    exp_X = np.exp(X)\n","    exp_X_sum = np.sum(exp_X,axis=1).reshape(-1,1)\n","    exp_X = exp_X/exp_X_sum\n","    return exp_X\n","\n","#derivative of tanh\n","def tanh_derivative(X):\n","    return 1-(X**2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N_Q9BTVWcU0L","colab_type":"code","colab":{}},"source":["#initialize parameters\n","def initialize_parameters():\n","    #initialize the parameters with 0 mean and 0.01 standard deviation\n","    mean = 0\n","    std = 0.01\n","    \n","    #lstm cell weights\n","    forget_gate_weights = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))\n","    input_gate_weights  = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))\n","    output_gate_weights = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))\n","    gate_gate_weights   = np.random.normal(mean,std,(input_units+hidden_units,hidden_units))\n","    \n","    #hidden to output weights (output cell)\n","    hidden_output_weights = np.random.normal(mean,std,(hidden_units,output_units))\n","    \n","    parameters = dict()\n","    parameters['fgw'] = forget_gate_weights\n","    parameters['igw'] = input_gate_weights\n","    parameters['ogw'] = output_gate_weights\n","    parameters['ggw'] = gate_gate_weights\n","    parameters['how'] = hidden_output_weights\n","    \n","    return parameters"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h2wuvyP6cWEK","colab_type":"code","colab":{}},"source":["#single lstm cell\n","def lstm_cell(batch_dataset, prev_activation_matrix, prev_cell_matrix, parameters):\n","    #get parameters\n","    fgw = parameters['fgw']\n","    igw = parameters['igw']\n","    ogw = parameters['ogw']\n","    ggw = parameters['ggw']\n","    \n","    #concat batch data and prev_activation matrix\n","    concat_dataset = np.concatenate((batch_dataset,prev_activation_matrix),axis=1)\n","    \n","    #forget gate activations\n","    fa = np.matmul(concat_dataset,fgw)\n","    fa = sigmoid(fa)\n","    \n","    #input gate activations\n","    ia = np.matmul(concat_dataset,igw)\n","    ia = sigmoid(ia)\n","    \n","    #output gate activations\n","    oa = np.matmul(concat_dataset,ogw)\n","    oa = sigmoid(oa)\n","    \n","    #gate gate activations\n","    ga = np.matmul(concat_dataset,ggw)\n","    ga = tanh_activation(ga)\n","    \n","    #new cell memory matrix\n","    cell_memory_matrix = np.multiply(fa,prev_cell_matrix) + np.multiply(ia,ga)\n","    \n","    #current activation matrix\n","    activation_matrix = np.multiply(oa, tanh_activation(cell_memory_matrix))\n","    \n","    #lets store the activations to be used in back prop\n","    lstm_activations = dict()\n","    lstm_activations['fa'] = fa\n","    lstm_activations['ia'] = ia\n","    lstm_activations['oa'] = oa\n","    lstm_activations['ga'] = ga\n","    \n","    return lstm_activations,cell_memory_matrix,activation_matrix"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f9PwZf_VcbEj","colab_type":"code","colab":{}},"source":["def output_cell(activation_matrix,parameters):\n","    #get hidden to output parameters\n","    how = parameters['how']\n","    \n","    #get outputs \n","    output_matrix = np.matmul(activation_matrix,how)\n","    output_matrix = softmax(output_matrix)\n","    \n","    return output_matrix"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ol_mQ5MbcdTL","colab_type":"code","colab":{}},"source":["def get_embeddings(batch_dataset,embeddings):\n","    embedding_dataset = np.matmul(batch_dataset,embeddings)\n","    return embedding_dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6tyg9lQ8cfTk","colab_type":"code","colab":{}},"source":["#forward propagation\n","def forward_propagation(batches,parameters,embeddings):\n","    #get batch size\n","    batch_size = batches[0].shape[0]\n","    \n","    #to store the activations of all the unrollings.\n","    lstm_cache = dict()                 #lstm cache\n","    activation_cache = dict()           #activation cache \n","    cell_cache = dict()                 #cell cache\n","    output_cache = dict()               #output cache\n","    embedding_cache = dict()            #embedding cache \n","    \n","    #initial activation_matrix(a0) and cell_matrix(c0)\n","    a0 = np.zeros([batch_size,hidden_units],dtype=np.float32)\n","    c0 = np.zeros([batch_size,hidden_units],dtype=np.float32)\n","    \n","    #store the initial activations in cache\n","    activation_cache['a0'] = a0\n","    cell_cache['c0'] = c0\n","    \n","    #unroll the names\n","    for i in range(len(batches)-1):\n","        #get first first character batch\n","        batch_dataset = batches[i]\n","        \n","        #get embeddings \n","        batch_dataset = get_embeddings(batch_dataset,embeddings)\n","        embedding_cache['emb'+str(i)] = batch_dataset\n","        \n","        #lstm cell\n","        lstm_activations,ct,at = lstm_cell(batch_dataset,a0,c0,parameters)\n","        \n","        #output cell\n","        ot = output_cell(at,parameters)\n","        \n","        #store the time 't' activations in caches\n","        lstm_cache['lstm' + str(i+1)]  = lstm_activations\n","        activation_cache['a'+str(i+1)] = at\n","        cell_cache['c' + str(i+1)] = ct\n","        output_cache['o'+str(i+1)] = ot\n","        \n","        #update a0 and c0 to new 'at' and 'ct' for next lstm cell\n","        a0 = at\n","        c0 = ct\n","        \n","    return embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ANbaOzB6cjJl","colab_type":"code","colab":{}},"source":["#calculate loss, perplexity and accuracy\n","def cal_loss_accuracy(batch_labels,output_cache):\n","    loss = 0  #to sum loss for each time step\n","    acc  = 0  #to sum acc for each time step \n","    prob = 1  #probability product of each time step predicted char\n","    \n","    #batch size\n","    batch_size = batch_labels[0].shape[0]\n","    \n","    #loop through each time step\n","    for i in range(1,len(output_cache)+1):\n","        #get true labels and predictions\n","        labels = batch_labels[i]\n","        pred = output_cache['o'+str(i)]\n","        \n","        prob = np.multiply(prob,np.sum(np.multiply(labels,pred),axis=1).reshape(-1,1))\n","        loss += np.sum((np.multiply(labels,np.log(pred)) + np.multiply(1-labels,np.log(1-pred))),axis=1).reshape(-1,1)\n","        acc  += np.array(np.argmax(labels,1)==np.argmax(pred,1),dtype=np.float32).reshape(-1,1)\n","    \n","    #calculate perplexity loss and accuracy\n","    perplexity = np.sum((1/prob)**(1/len(output_cache)))/batch_size\n","    loss = np.sum(loss)*(-1/batch_size)\n","    acc  = np.sum(acc)/(batch_size)\n","    acc = acc/len(output_cache)\n","    \n","    return perplexity,loss,acc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lr-Z3oEPcmAO","colab_type":"code","colab":{}},"source":["#calculate output cell errors\n","def calculate_output_cell_error(batch_labels,output_cache,parameters):\n","    #to store the output errors for each time step\n","    output_error_cache = dict()\n","    activation_error_cache = dict()\n","    how = parameters['how']\n","    \n","    #loop through each time step\n","    for i in range(1,len(output_cache)+1):\n","        #get true and predicted labels\n","        labels = batch_labels[i]\n","        pred = output_cache['o'+str(i)]\n","        \n","        #calculate the output_error for time step 't'\n","        error_output = pred - labels\n","        \n","        #calculate the activation error for time step 't'\n","        error_activation = np.matmul(error_output,how.T)\n","        \n","        #store the output and activation error in dict\n","        output_error_cache['eo'+str(i)] = error_output\n","        activation_error_cache['ea'+str(i)] = error_activation\n","        \n","    return output_error_cache,activation_error_cache"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rkLds37ncq_N","colab_type":"code","colab":{}},"source":["#calculate error for single lstm cell\n","def calculate_single_lstm_cell_error(activation_output_error,next_activation_error,next_cell_error,parameters,lstm_activation,cell_activation,prev_cell_activation):\n","    #activation error =  error coming from output cell and error coming from the next lstm cell\n","    activation_error = activation_output_error + next_activation_error\n","    \n","    #output gate error\n","    oa = lstm_activation['oa']\n","    eo = np.multiply(activation_error,tanh_activation(cell_activation))\n","    eo = np.multiply(np.multiply(eo,oa),1-oa)\n","    \n","    #cell activation error\n","    cell_error = np.multiply(activation_error,oa)\n","    cell_error = np.multiply(cell_error,tanh_derivative(tanh_activation(cell_activation)))\n","    #error also coming from next lstm cell \n","    cell_error += next_cell_error\n","    \n","    #input gate error\n","    ia = lstm_activation['ia']\n","    ga = lstm_activation['ga']\n","    ei = np.multiply(cell_error,ga)\n","    ei = np.multiply(np.multiply(ei,ia),1-ia)\n","    \n","    #gate gate error\n","    eg = np.multiply(cell_error,ia)\n","    eg = np.multiply(eg,tanh_derivative(ga))\n","    \n","    #forget gate error\n","    fa = lstm_activation['fa']\n","    ef = np.multiply(cell_error,prev_cell_activation)\n","    ef = np.multiply(np.multiply(ef,fa),1-fa)\n","    \n","    #prev cell error\n","    prev_cell_error = np.multiply(cell_error,fa)\n","    \n","    #get parameters\n","    fgw = parameters['fgw']\n","    igw = parameters['igw']\n","    ggw = parameters['ggw']\n","    ogw = parameters['ogw']\n","    \n","    #embedding + hidden activation error\n","    embed_activation_error = np.matmul(ef,fgw.T)\n","    embed_activation_error += np.matmul(ei,igw.T)\n","    embed_activation_error += np.matmul(eo,ogw.T)\n","    embed_activation_error += np.matmul(eg,ggw.T)\n","    \n","    input_hidden_units = fgw.shape[0]\n","    hidden_units = fgw.shape[1]\n","    input_units = input_hidden_units - hidden_units\n","    \n","    #prev activation error\n","    prev_activation_error = embed_activation_error[:,input_units:]\n","    \n","    #input error (embedding error)\n","    embed_error = embed_activation_error[:,:input_units]\n","    \n","    #store lstm error\n","    lstm_error = dict()\n","    lstm_error['ef'] = ef\n","    lstm_error['ei'] = ei\n","    lstm_error['eo'] = eo\n","    lstm_error['eg'] = eg\n","    \n","    return prev_activation_error,prev_cell_error,embed_error,lstm_error"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9PGKcISocswU","colab_type":"code","colab":{}},"source":["#calculate output cell derivatives\n","def calculate_output_cell_derivatives(output_error_cache,activation_cache,parameters):\n","    #to store the sum of derivatives from each time step\n","    dhow = np.zeros(parameters['how'].shape)\n","    \n","    batch_size = activation_cache['a1'].shape[0]\n","    \n","    #loop through the time steps \n","    for i in range(1,len(output_error_cache)+1):\n","        #get output error\n","        output_error = output_error_cache['eo' + str(i)]\n","        \n","        #get input activation\n","        activation = activation_cache['a'+str(i)]\n","        \n","        #cal derivative and summing up!\n","        dhow += np.matmul(activation.T,output_error)/batch_size\n","        \n","    return dhow"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xkIrGcf5cwgd","colab_type":"code","colab":{}},"source":["#calculate derivatives for single lstm cell\n","def calculate_single_lstm_cell_derivatives(lstm_error,embedding_matrix,activation_matrix):\n","    #get error for single time step\n","    ef = lstm_error['ef']\n","    ei = lstm_error['ei']\n","    eo = lstm_error['eo']\n","    eg = lstm_error['eg']\n","    \n","    #get input activations for this time step\n","    concat_matrix = np.concatenate((embedding_matrix,activation_matrix),axis=1)\n","    \n","    batch_size = embedding_matrix.shape[0]\n","    \n","    #cal derivatives for this time step\n","    dfgw = np.matmul(concat_matrix.T,ef)/batch_size\n","    digw = np.matmul(concat_matrix.T,ei)/batch_size\n","    dogw = np.matmul(concat_matrix.T,eo)/batch_size\n","    dggw = np.matmul(concat_matrix.T,eg)/batch_size\n","    \n","    #store the derivatives for this time step in dict\n","    derivatives = dict()\n","    derivatives['dfgw'] = dfgw\n","    derivatives['digw'] = digw\n","    derivatives['dogw'] = dogw\n","    derivatives['dggw'] = dggw\n","    \n","    return derivatives"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YyREBKEqcyS9","colab_type":"code","colab":{}},"source":["#backpropagation\n","def backward_propagation(batch_labels,embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache,parameters):\n","    #calculate output errors \n","    output_error_cache,activation_error_cache = calculate_output_cell_error(batch_labels,output_cache,parameters)\n","    \n","    #to store lstm error for each time step\n","    lstm_error_cache = dict()\n","    \n","    #to store embeding errors for each time step\n","    embedding_error_cache = dict()\n","    \n","    # next activation error \n","    # next cell error  \n","    #for last cell will be zero\n","    eat = np.zeros(activation_error_cache['ea1'].shape)\n","    ect = np.zeros(activation_error_cache['ea1'].shape)\n","    \n","    #calculate all lstm cell errors (going from last time-step to the first time step)\n","    for i in range(len(lstm_cache),0,-1):\n","        #calculate the lstm errors for this time step 't'\n","        pae,pce,ee,le = calculate_single_lstm_cell_error(activation_error_cache['ea'+str(i)],eat,ect,parameters,lstm_cache['lstm'+str(i)],cell_cache['c'+str(i)],cell_cache['c'+str(i-1)])\n","        \n","        #store the lstm error in dict\n","        lstm_error_cache['elstm'+str(i)] = le\n","        \n","        #store the embedding error in dict\n","        embedding_error_cache['eemb'+str(i-1)] = ee\n","        \n","        #update the next activation error and next cell error for previous cell\n","        eat = pae\n","        ect = pce\n","    \n","    \n","    #calculate output cell derivatives\n","    derivatives = dict()\n","    derivatives['dhow'] = calculate_output_cell_derivatives(output_error_cache,activation_cache,parameters)\n","    \n","    #calculate lstm cell derivatives for each time step and store in lstm_derivatives dict\n","    lstm_derivatives = dict()\n","    for i in range(1,len(lstm_error_cache)+1):\n","        lstm_derivatives['dlstm'+str(i)] = calculate_single_lstm_cell_derivatives(lstm_error_cache['elstm'+str(i)],embedding_cache['emb'+str(i-1)],activation_cache['a'+str(i-1)])\n","    \n","    #initialize the derivatives to zeros \n","    derivatives['dfgw'] = np.zeros(parameters['fgw'].shape)\n","    derivatives['digw'] = np.zeros(parameters['igw'].shape)\n","    derivatives['dogw'] = np.zeros(parameters['ogw'].shape)\n","    derivatives['dggw'] = np.zeros(parameters['ggw'].shape)\n","    \n","    #sum up the derivatives for each time step\n","    for i in range(1,len(lstm_error_cache)+1):\n","        derivatives['dfgw'] += lstm_derivatives['dlstm'+str(i)]['dfgw']\n","        derivatives['digw'] += lstm_derivatives['dlstm'+str(i)]['digw']\n","        derivatives['dogw'] += lstm_derivatives['dlstm'+str(i)]['dogw']\n","        derivatives['dggw'] += lstm_derivatives['dlstm'+str(i)]['dggw']\n","    \n","    return derivatives,embedding_error_cache"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PS_cuS9Nc60F","colab_type":"code","colab":{}},"source":["#update the parameters using adam optimizer\n","#adam optimization\n","def update_parameters(parameters,derivatives,V,S,t):\n","    #get derivatives\n","    dfgw = derivatives['dfgw']\n","    digw = derivatives['digw']\n","    dogw = derivatives['dogw']\n","    dggw = derivatives['dggw']\n","    dhow = derivatives['dhow']\n","    \n","    #get parameters\n","    fgw = parameters['fgw']\n","    igw = parameters['igw']\n","    ogw = parameters['ogw']\n","    ggw = parameters['ggw']\n","    how = parameters['how']\n","    \n","    #get V parameters\n","    vfgw = V['vfgw']\n","    vigw = V['vigw']\n","    vogw = V['vogw']\n","    vggw = V['vggw']\n","    vhow = V['vhow']\n","    \n","    #get S parameters\n","    sfgw = S['sfgw']\n","    sigw = S['sigw']\n","    sogw = S['sogw']\n","    sggw = S['sggw']\n","    show = S['show']\n","    \n","    #calculate the V parameters from V and current derivatives\n","    vfgw = (beta1*vfgw + (1-beta1)*dfgw)\n","    vigw = (beta1*vigw + (1-beta1)*digw)\n","    vogw = (beta1*vogw + (1-beta1)*dogw)\n","    vggw = (beta1*vggw + (1-beta1)*dggw)\n","    vhow = (beta1*vhow + (1-beta1)*dhow)\n","    \n","    #calculate the S parameters from S and current derivatives\n","    sfgw = (beta2*sfgw + (1-beta2)*(dfgw**2))\n","    sigw = (beta2*sigw + (1-beta2)*(digw**2))\n","    sogw = (beta2*sogw + (1-beta2)*(dogw**2))\n","    sggw = (beta2*sggw + (1-beta2)*(dggw**2))\n","    show = (beta2*show + (1-beta2)*(dhow**2))\n","    \n","    #update the parameters\n","    fgw = fgw - learning_rate*((vfgw)/(np.sqrt(sfgw) + 1e-6))\n","    igw = igw - learning_rate*((vigw)/(np.sqrt(sigw) + 1e-6))\n","    ogw = ogw - learning_rate*((vogw)/(np.sqrt(sogw) + 1e-6))\n","    ggw = ggw - learning_rate*((vggw)/(np.sqrt(sggw) + 1e-6))\n","    how = how - learning_rate*((vhow)/(np.sqrt(show) + 1e-6))\n","    \n","    #store the new weights\n","    parameters['fgw'] = fgw\n","    parameters['igw'] = igw\n","    parameters['ogw'] = ogw\n","    parameters['ggw'] = ggw\n","    parameters['how'] = how\n","    \n","    #store the new V parameters\n","    V['vfgw'] = vfgw \n","    V['vigw'] = vigw \n","    V['vogw'] = vogw \n","    V['vggw'] = vggw\n","    V['vhow'] = vhow\n","    \n","    #store the s parameters\n","    S['sfgw'] = sfgw \n","    S['sigw'] = sigw \n","    S['sogw'] = sogw \n","    S['sggw'] = sggw\n","    S['show'] = show\n","    \n","    return parameters,V,S"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"81GyltJRc8Yd","colab_type":"code","colab":{}},"source":["#update the Embeddings\n","def update_embeddings(embeddings,embedding_error_cache,batch_labels):\n","    #to store the embeddings derivatives\n","    embedding_derivatives = np.zeros(embeddings.shape)\n","    \n","    batch_size = batch_labels[0].shape[0]\n","    \n","    #sum the embedding derivatives for each time step\n","    for i in range(len(embedding_error_cache)):\n","        embedding_derivatives += np.matmul(batch_labels[i].T,embedding_error_cache['eemb'+str(i)])/batch_size\n","    \n","    #update the embeddings\n","    embeddings = embeddings - learning_rate*embedding_derivatives\n","    return embeddings"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8rL8CoiOc-_2","colab_type":"code","colab":{}},"source":["def initialize_V(parameters):\n","    Vfgw = np.zeros(parameters['fgw'].shape)\n","    Vigw = np.zeros(parameters['igw'].shape)\n","    Vogw = np.zeros(parameters['ogw'].shape)\n","    Vggw = np.zeros(parameters['ggw'].shape)\n","    Vhow = np.zeros(parameters['how'].shape)\n","    \n","    V = dict()\n","    V['vfgw'] = Vfgw\n","    V['vigw'] = Vigw\n","    V['vogw'] = Vogw\n","    V['vggw'] = Vggw\n","    V['vhow'] = Vhow\n","    return V\n","\n","def initialize_S(parameters):\n","    Sfgw = np.zeros(parameters['fgw'].shape)\n","    Sigw = np.zeros(parameters['igw'].shape)\n","    Sogw = np.zeros(parameters['ogw'].shape)\n","    Sggw = np.zeros(parameters['ggw'].shape)\n","    Show = np.zeros(parameters['how'].shape)\n","    \n","    S = dict()\n","    S['sfgw'] = Sfgw\n","    S['sigw'] = Sigw\n","    S['sogw'] = Sogw\n","    S['sggw'] = Sggw\n","    S['show'] = Show\n","    return S"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lryjLVK-dByx","colab_type":"code","colab":{}},"source":["#train function\n","def train(train_dataset,iters=1000,batch_size=20):\n","    #initalize the parameters\n","    parameters = initialize_parameters()\n","    \n","    #initialize the V and S parameters for Adam\n","    V = initialize_V(parameters)\n","    S = initialize_S(parameters)\n","    \n","    #generate the random embeddings\n","    embeddings = np.random.normal(0,0.01,(len(vocab),input_units))\n","    \n","    #to store the Loss, Perplexity and Accuracy for each batch\n","    J = []\n","    P = []\n","    A = []\n","    \n","    \n","    for step in range(iters):\n","        #get batch dataset\n","        index = step%len(train_dataset)\n","        batches = train_dataset[index]\n","        \n","        #forward propagation\n","        embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache = forward_propagation(batches,parameters,embeddings)\n","        \n","        #calculate the loss, perplexity and accuracy\n","        perplexity,loss,acc = cal_loss_accuracy(batches,output_cache)\n","        \n","        #backward propagation\n","        derivatives,embedding_error_cache = backward_propagation(batches,embedding_cache,lstm_cache,activation_cache,cell_cache,output_cache,parameters)\n","        \n","        #update the parameters\n","        parameters,V,S = update_parameters(parameters,derivatives,V,S,step)\n","        \n","        #update the embeddings\n","        embeddings = update_embeddings(embeddings,embedding_error_cache,batches)\n","        \n","        \n","        J.append(loss)\n","        P.append(perplexity)\n","        A.append(acc)\n","        \n","        #print loss, accuracy and perplexity\n","        if(step%1000==0):\n","            print(\"For Single Batch :\")\n","            print('Step       = {}'.format(step))\n","            print('Loss       = {}'.format(round(loss,2)))\n","            print('Perplexity = {}'.format(round(perplexity,2)))\n","            print('Accuracy   = {}'.format(round(acc*100,2)))\n","            print()\n","    \n","    return embeddings, parameters,J,P,A"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZEiNDee2dFje","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"692784c6-b868-430f-b6e9-acc3b96f0b4c"},"source":["embeddings,parameters,J,P,A = train(train_dataset,iters=8001)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["For Single Batch :\n","Step       = 0\n","Loss       = 47.05\n","Perplexity = 27.0\n","Accuracy   = 1.82\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4NGwmngodgPx","colab_type":"code","colab":{}},"source":["avg_loss = list()\n","avg_acc = list()\n","avg_perp = list()\n","i = 0\n","while(i<len(J)):\n","    avg_loss.append(np.mean(J[i:i+30]))\n","    avg_acc.append(np.mean(A[i:i+30]))\n","    avg_perp.append(np.mean(P[i:i+30]))\n","    i += 30\n","\n","plt.plot(list(range(len(avg_loss))),avg_loss)\n","plt.xlabel(\"x\")\n","plt.ylabel(\"Loss (Avg of 30 batches)\")\n","plt.title(\"Loss Graph\")\n","plt.show()\n","\n","plt.plot(list(range(len(avg_perp))),avg_perp)\n","plt.xlabel(\"x\")\n","plt.ylabel(\"Perplexity (Avg of 30 batches)\")\n","plt.title(\"Perplexity Graph\")\n","plt.show()\n","\n","plt.plot(list(range(len(avg_acc))),avg_acc)\n","plt.xlabel(\"x\")\n","plt.ylabel(\"Accuracy (Avg of 30 batches)\")\n","plt.title(\"Accuracy Graph\")\n","plt.show()    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dYfP7chmdjrX","colab_type":"code","colab":{}},"source":["#predict\n","def predict(parameters,embeddings,id_char,vocab_size):\n","    #to store some predicted names\n","    names = []\n","    \n","    #predict 20 names\n","    for i in range(20):\n","        #initial activation_matrix(a0) and cell_matrix(c0)\n","        a0 = np.zeros([1,hidden_units],dtype=np.float32)\n","        c0 = np.zeros([1,hidden_units],dtype=np.float32)\n","\n","        #initalize blank name\n","        name = ''\n","        \n","        #make a batch dataset of single char\n","        batch_dataset = np.zeros([1,vocab_size])\n","        \n","        #get random start character\n","        index = np.random.randint(0,27,1)[0]\n","        \n","        #make that index 1.0\n","        batch_dataset[0,index] = 1.0\n","        \n","        #add first char to name\n","        name += id_char[index]\n","        \n","        #get char from id_char dict\n","        char = id_char[index]\n","        \n","        #loop until algo predicts '.'\n","        while(char!='.'):\n","            #get embeddings\n","            batch_dataset = get_embeddings(batch_dataset,embeddings)\n","\n","            #lstm cell\n","            lstm_activations,ct,at = lstm_cell(batch_dataset,a0,c0,parameters)\n","\n","            #output cell\n","            ot = output_cell(at,parameters)\n","            \n","            #either select random.choice ot np.argmax\n","            pred = np.random.choice(27,1,p=ot[0])[0]\n","            \n","            #get predicted char index\n","            #pred = np.argmax(ot)\n","                \n","            #add char to name\n","            name += id_char[pred]\n","            \n","            char = id_char[pred]\n","            \n","            #change the batch_dataset to this new predicted char\n","            batch_dataset = np.zeros([1,vocab_size])\n","            batch_dataset[0,pred] = 1.0\n","\n","            #update a0 and c0 to new 'at' and 'ct' for next lstm cell\n","            a0 = at\n","            c0 = ct\n","            \n","        #append the predicted name to names list\n","        names.append(name)\n","        \n","    return names"],"execution_count":0,"outputs":[]}]}